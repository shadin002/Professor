{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpfv9PloAN7s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n"
      ],
      "metadata": {
        "id": "MLT6GOjJAdIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "model.fc = nn.Identity()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU0DQJjNj6Oz",
        "outputId": "2626cce0-0f4c-4bf9-802b-2f5d67b7386a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 176MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load image from the path\n",
        "img = Image.open(\"/content/flower1.jpg\")\n",
        "#img = Image.open(\"/content/lion.jpg\")\n",
        "\n",
        "# Transform image as resnet expect 224*224\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor() #Converts image to tensor\n",
        "])\n",
        "\n",
        "img_tensor = transform(img)\n",
        "\n",
        "# Add batch dimension\n",
        "x = img_tensor.unsqueeze(0)   # shape: [1, 3, 224, 224] converts [3, 224, 224] ->[1, 3, 224, 224]\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.eval()\n",
        "model.fc = nn.Identity()\n",
        "\n",
        "# Extract features\n",
        "with torch.no_grad():\n",
        "    features = model(x)\n",
        "\n",
        "features.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcoT8iKIkebD",
        "outputId": "7cb0631e-8c8c-4771-b43c-2b231ea0c8c9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(features.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH5lTYGOnQTE",
        "outputId": "3cc7aa7a-421a-492c-c60f-d2bf941f9d42"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features1 = features.squeeze()\n",
        "print(features1[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y0Apvn2oleS",
        "outputId": "f656cea8-7067-44fd-c9fe-b6afb8cf83d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6247, 0.0351, 2.2836, 3.6823, 2.3098, 0.4677, 0.6723, 5.0244, 1.7302,\n",
            "        0.2080])\n"
          ]
        }
      ]
    }
  ]
}